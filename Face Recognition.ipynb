{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import argparse\n",
    "import time\n",
    "from imutils.video import FPS\n",
    "from imutils.video import VideoStream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Face Detector...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Face Detector...\")\n",
    "protoPath = \"face_detection_model/deploy.prototxt\"\n",
    "modelPath = \"face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "imagePaths = list(paths.list_images(\"dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea20f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Face Detector...\n",
      "Loading Face Recognizer...\n",
      "Quantifying Faces...\n",
      "Processing image 0/200\n",
      "Processing image 50/200\n",
      "Processing image 100/200\n",
      "Processing image 150/200\n",
      "[INFO] serializing 200 encodings...\n"
     ]
    }
   ],
   "source": [
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "print(\"Loading Face Recognizer...\")\n",
    "embedder = cv2.dnn.readNetFromTorch(\"openface_nn4.small2.v1.t7\")\n",
    "print(\"Quantifying Faces...\")\n",
    "knownEmbeddings = []\n",
    "knownNames = []\n",
    "total = 0\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "\tif (i%50 == 0):\n",
    "\t\tprint(\"Processing image {}/{}\".format(i, len(imagePaths)))\n",
    "\tname = imagePath.split(os.path.sep)[-2]\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = imutils.resize(image, width=600)\n",
    "\t(h, w) = image.shape[:2]\n",
    "\n",
    "\t# construct a blob from the image\n",
    "\timageBlob = cv2.dnn.blobFromImage(\n",
    "\t\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\tdetector.setInput(imageBlob)\n",
    "\tdetections = detector.forward()\n",
    "\n",
    "\tif len(detections) > 0:\n",
    "\t\t\n",
    "\t\ti = np.argmax(detections[0, 0, :, 2])\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t\n",
    "\t\tif confidence > 0.5:\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\tface = image[startY:endY, startX:endX]\n",
    "\t\t\t(fH, fW) = face.shape[:2]\n",
    "\n",
    "\t\t\tif fW < 20 or fH < 20:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "\t\t\t\t(96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "\t\t\tembedder.setInput(faceBlob)\n",
    "\t\t\tvec = embedder.forward()\n",
    "\n",
    "\t\t\tknownNames.append(name)\n",
    "\t\t\tknownEmbeddings.append(vec.flatten())\n",
    "\t\t\ttotal += 1\n",
    "\n",
    "# dump the facial embeddings + names to disk\n",
    "print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "f = open(\"output/embeddings.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa77285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face embeddings...\n",
      "[INFO] encoding labels...\n",
      "[INFO] training model...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading face embeddings...\")\n",
    "data = pickle.loads(open(\"output/embeddings.pickle\", \"rb\").read())\n",
    "\n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data[\"names\"])\n",
    "print(\"[INFO] training model...\")\n",
    "recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "recognizer.fit(data[\"embeddings\"], labels)\n",
    "f = open(\"output/recognizer\", \"wb\")\n",
    "f.write(pickle.dumps(recognizer))\n",
    "f.close()\n",
    "f = open(\"output/le.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea9d919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Face Detector...\n",
      "Loading Face Recognizer...\n",
      "Starting Video Stream...\n",
      "Elasped time: 57.64\n",
      "Approx. FPS: 12.16\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Face Detector...\")\n",
    "protoPath = \"face_detection_model/deploy.prototxt\"\n",
    "modelPath = \"face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "# load serialized face embedding model\n",
    "print(\"Loading Face Recognizer...\")\n",
    "embedder = cv2.dnn.readNetFromTorch(\"openface_nn4.small2.v1.t7\")\n",
    "\n",
    "# load the actual face recognition model along with the label encoder\n",
    "recognizer = pickle.loads(open(\"output/recognizer\", \"rb\").read())\n",
    "le = pickle.loads(open(\"output/le.pickle\", \"rb\").read())\n",
    "\n",
    "# initialize the video stream, then allow the camera sensor to warm up\n",
    "print(\"Starting Video Stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# start the FPS throughput estimator\n",
    "fps = FPS().start()\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "\t# grab the frame from the threaded video stream\n",
    "\tframe = vs.read()\n",
    "\n",
    "\t# resize the frame to have a width of 600 pixels (while maintaining the aspect ratio), and then grab the image dimensions\n",
    "\tframe = imutils.resize(frame, width=600)\n",
    "\t(h, w) = frame.shape[:2]\n",
    "\n",
    "\t# construct a blob from the image\n",
    "\timageBlob = cv2.dnn.blobFromImage(\n",
    "\t\tcv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "\t# apply OpenCV's deep learning-based face detector to localize faces in the input image\n",
    "\tdetector.setInput(imageBlob)\n",
    "\tdetections = detector.forward()\n",
    "\n",
    "\t# loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with the prediction\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t# filter out weak detections\n",
    "\t\tif confidence > 0.5:\n",
    "\t\t\t# compute the (x, y)-coordinates of the bounding box for the face\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# extract the face ROI\n",
    "\t\t\tface = frame[startY:endY, startX:endX]\n",
    "\t\t\t(fH, fW) = face.shape[:2]\n",
    "\n",
    "\t\t\t# ensure the face width and height are sufficiently large\n",
    "\t\t\tif fW < 20 or fH < 20:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# construct a blob for the face ROI, then pass the blob through our face embedding model to obtain the 128-d quantification of the face\n",
    "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "\t\t\t\t(96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "\t\t\tembedder.setInput(faceBlob)\n",
    "\t\t\tvec = embedder.forward()\n",
    "\n",
    "\t\t\t# perform classification to recognize the face\n",
    "\t\t\tpreds = recognizer.predict_proba(vec)[0]\n",
    "\t\t\tj = np.argmax(preds)\n",
    "\t\t\tproba = preds[j]\n",
    "\t\t\tname = le.classes_[j]\n",
    "\n",
    "\t\t\t# draw the bounding box of the face along with the associated probability\n",
    "\t\t\ttext = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "\t\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "\t\t\t\t(0, 0, 255), 2)\n",
    "\t\t\tcv2.putText(frame, text, (startX, y),\n",
    "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "\t# update the FPS counter\n",
    "\tfps.update()\n",
    "\n",
    "\t# show the output frame\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"Elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"Approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec4eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def capture_images(output_folder, label, num_images=100):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Open the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Counter for the number of captured images\n",
    "    count = 0\n",
    "\n",
    "    while count < num_images:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Capture Images', frame)\n",
    "\n",
    "        # Save the image to the output folder\n",
    "        image_filename = os.path.join(output_folder, f\"{label}_{count}.png\")\n",
    "        cv2.imwrite(image_filename, frame)\n",
    "\n",
    "        # Increment the counter\n",
    "        count += 1\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam and close the OpenCV window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the output folder and label\n",
    "    output_folder = \"D:\\\\Mtech 2022-24\\\\Semester 3\\\\Project\\\\face-recognition-using-deep-learning\\\\dataset\\\\vatsal\"\n",
    "    label = \"vatsal\"  # Change this label as needed\n",
    "\n",
    "    # Specify the number of images to capture\n",
    "    num_images = 100\n",
    "\n",
    "    # Capture images\n",
    "    capture_images(output_folder, label, num_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Mtech 2022-24\\Semester 3\\Project\\face-recognition-using-deep-learning\\Face Recognition.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m vs \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# Capture frame from video stream\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m vs\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# Preprocess the frame (resize, convert to grayscale, etc.)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# Perform face detection\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Release video stream\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#X14sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m vs\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Phase\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# Load pre-trained face embeddings and names\n",
    "data = pickle.loads(open(\"output/embeddings.pickle\", \"rb\").read())\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data[\"names\"])\n",
    "\n",
    "# Train SVM model\n",
    "recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "recognizer.fit(data[\"embeddings\"], labels)\n",
    "\n",
    "# Save the trained model and label encoder\n",
    "with open(\"output/recognizer.pickle\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(recognizer))\n",
    "\n",
    "with open(\"output/le.pickle\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(le))\n",
    "\n",
    "# Recognition Phase\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load trained face recognition model and label encoder\n",
    "recognizer = pickle.loads(open(\"output/recognizer.pickle\", \"rb\").read())\n",
    "le = pickle.loads(open(\"output/le.pickle\", \"rb\").read())\n",
    "\n",
    "# Initialize video capture\n",
    "vs = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame from video stream\n",
    "    ret, frame = vs.read()\n",
    "\n",
    "    # Preprocess the frame (resize, convert to grayscale, etc.)\n",
    "\n",
    "    # Perform face detection\n",
    "\n",
    "    # Extract face region\n",
    "\n",
    "    # Preprocess face for embedding\n",
    "\n",
    "    # Pass face through embedding model\n",
    "\n",
    "    # Use trained recognizer to predict person's nameq\n",
    "\n",
    "    # Display bounding box and name on the frame\n",
    "\n",
    "    # Check for exit key press\n",
    "\n",
    "# Release video stream\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a37440d",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32md:\\Mtech 2022-24\\Semester 3\\Project\\face-recognition-using-deep-learning\\Face Recognition.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m face_cascade \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mCascadeClassifier(\u001b[39m'\u001b[39m\u001b[39mhaarcascade_frontalface_default.xml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m gray_frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m faces \u001b[39m=\u001b[39m face_cascade\u001b[39m.\u001b[39;49mdetectMultiScale(gray_frame, scaleFactor\u001b[39m=\u001b[39;49m\u001b[39m1.1\u001b[39;49m, minNeighbors\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Loop over detected faces\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m (x, y, w, h) \u001b[39min\u001b[39;00m faces:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mtech%202022-24/Semester%203/Project/face-recognition-using-deep-learning/Face%20Recognition.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Extract face region\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the trained face recognition model and label encoder\n",
    "recognizer = pickle.loads(open(\"output/recognizer.pickle\", \"rb\").read())\n",
    "le = pickle.loads(open(\"output/le.pickle\", \"rb\").read())\n",
    "\n",
    "# Initialize video capture\n",
    "vs = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame from the video stream\n",
    "    ret, frame = vs.read()\n",
    "\n",
    "    # Perform face detection using a pre-trained face detector\n",
    "    # This part needs to be replaced with an actual face detection mechanism\n",
    "    # For example, you can use OpenCV's pre-trained Haarcascades classifier\n",
    "    # https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    # Loop over detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face region\n",
    "        face = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Preprocess face for embedding\n",
    "        # This part needs to be replaced with your face preprocessing logic\n",
    "\n",
    "        # Pass face through the embedding model\n",
    "        # This part needs to be replaced with your face embedding logic\n",
    "\n",
    "        # Placeholder for the face embedding vector\n",
    "        vec = np.random.rand(128)  # Replace with actual embedding\n",
    "\n",
    "        # Use the trained recognizer to predict the person's name\n",
    "        preds = recognizer.predict_proba([vec])[0]\n",
    "        j = np.argmax(preds)\n",
    "        proba = preds[j]\n",
    "        name = le.classes_[j]\n",
    "\n",
    "        # Display bounding box and name on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        text = f\"{name}: {proba * 100:.2f}%\"\n",
    "        cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the output frame\n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "    # Check for exit key press\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release video stream\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c54b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1fc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
